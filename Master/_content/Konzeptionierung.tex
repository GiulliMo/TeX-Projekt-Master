\chapter{Konzeptionierung}
\label{ch: Konzeptionierung}

In diesem Kapitel wird die Konzeptionierung der Personenerkennung sowie die der Statemachine für das autonome Logistikfahrzeug erläutert. Wie auch in der vorangegangenen Bachelorarbeit geschah die Anforderungserhebung mit der Methode des \textit{Conceptual design specification technique for the engineering of complex Systems} (CONSENS). Am Institut für Systemtechnik der Hochschule Bochum wird zur systematischen Spezifikation von komplexen Systemen die \textit{CONSENS} Methode geschult. Die Anforderungen an die beschriebenen Teilsysteme wurden im Lastenheft als Anforderungsliste ... festgehalten. Die Komponenten für die Entwicklung sind im Umfeldmodell und den entsprechenden Wirkstrukturen dargestellt.
	
	
	\section{Anforderungserhebung mit CONSENS}
	\label{sec: Anforderungserhebung}
	
	In Abbildung \ref{fig: consensenv} ist das erweiterte Umfeldmodell des ALFs zu sehen. Das ursprüngliche Umfeldmodell ist in der entsprechenden Bachelorarbeit wiederzufinden \cite{Bachelorarbeit}. Der Aufbau des Entwurfs besteht im Allgemeinen aus hellblauen und gelben Hexagons, die Wirk- und Umfeldelemente repräsentieren. Der Informationsfluss zwischen den Elementen ist durch gestrichelte Pfeile gekennzeichnet. Das ALF gilt als Kern des Gesamtsystems und ist deswegen im Modell mittig dargestellt. Es interagiert sowohl mit Wirk- als auch mit Umfeldelementen, wie in Abbildung \ref{fig: consensenv} gezeigt. Der Zustandsautomat ist ein Teilsystem, das als Erweiterung des Umfelds gilt. Die von Herrn Dittmann entwickelte kann Sprachverarbeitung sowohl auf dem Rechner des ALFs als auch auf dem integrierten \textit{Raspberry Pi} ausgeführt werden \cite{Dittmann}. Transitionsbedingungen können dementsprechend je nach Anwendungsfall durch beide Hardwarekomponenten intern über das \textit{ROS}-Netzwerk an den EA gesendet.       \\
	
	\input{Bilder/umfeldmodell}	
	
	Die Erweiterung um die Peronenerkennung ist in der in Abbildung \ref{fig: consensctrl} gezeigten Wirkstruktur dargestellt. Eine Wirkstruktur repräsentiert den Inhalt eines Wirkelements und dient zum besseren Verständnis komplexer Zusammenhänge. In diesem Fall wird das Wirkelement mit dem Titel ALF aus dem vorangegangenem Umfeldmodell \ref{fig: consensenv} aufgeschlüsselt. Die hier gezeigte Wirkstruktur wurde um die Personenerkennung und die Sprachverarbeitung erweitert. Elemente mit blassen Farben sind für diese Masterarbeit eher unwichtig und werden weiterhin nicht behandelt.\\
	
	ür den Betrieb der Personenerkennung sind die Bildinformationen der integrierten \textit{Kinect}-Kameras notwendig. Als Ausgabe werden zweidimensionale Positionen von erkannten Personen für das Visualisierungsprogramm \textit{RViz} veröffentlicht. Für eine mögliche Interaktion mit anwesenden Personen werden Statusinformationen, wie oben links in Abbildung \ref{fig: consensctrl} dargestellt, ausgegeben werden. Die Ausgabe der Sprachverarbeitung hat durch die Klassifikation der Sprache einen Einfluss auf den Zustandsautomaten und ist somit für das Projekt relevant. Im Umfeldmodell wird die Klassifikation als Transitionsbedingung interpretiert und ist dort als solche gekennzeichnet. Die Sprachverarbeitung analysiert vom Benutzer gesprochene Sätze und extrahiert die Intention des Benutzers. Beispielsweise würde der Satz "Drive to position Alpha" die Transitionsbedingungen \textit{drive} und \textit{autonomous} hervorrufen. \\
		
	\input{Bilder/wirknav}
	
	
	
	\section{Konzept und Aufbau der Personenerkennung}
	\label{sec: Konzept Personenerkennung}
	
	Die Personenerkennung gilt als eine von zwei in dieser Masterarbeit entwickelten Erweiterungen und gleichzeitig als Hauptthema. Grundlegend werden Personen eindeutig unterschieden und wiedererkannt. Dies stellt die Grundlage einer optischen Interaktion sowie einer Kommunikation mit dem Menschen dar. In diesem Kapitel wird die Personenerkennung in ihrer Struktur und Umsetzung näher erläutert. Weiterhin wird die Auswahl entsprechender Softwarekomponenten gegenübergestellt und somit begründet.\\
	
	Grundlage für eine Personenerkennung ist die eindeutige Identifikation. Folglich muss ein System in der Lage sein äußerliche Merkmale festzustellen, die langfristig wiederzuerkennen sind. Wie bereits in Kapitel \ref{sec: Zielsetzung} beschrieben, sollen Personen über Jahre hinweg wiedererkannt werden. Aufgrund dessen konnte das generelle äußere Erscheinungsbild der Person wie zum Beispiel die Kleidung oder die Körperhaltung einer Person als Identifikationsmerkmal ausgeschlossen werden. Derartige Merkmale ändern sich täglich oder sogar minütlich und können somit nicht sicher zugeordnet werden. Das Gesicht eines Menschen kann sich je nach Alter schnell oder langsam verändern. Wird die Benutzergruppe des ALFs auf Menschen zwischen 20 und 70 Jahren beschränkt könnten individuelle Personen schätzungsweise mindestens 5 Jahre durch das Gesicht erkannt werden. Die eindeutige Erkennung setzt also vorraus, dass der Kopf einer Personen mit dem Gesicht zur Kamera des ALFs gerichtet ist. \\ 
	
		\subsection{Wirkstruktur der Personenerkennung}
		\label{subsec: Wirkstrukur Personenerkennung}
		
		In Abbildung \ref{fig: consenspers} ist die Wirkstruktur der Personenerkennung dargestellt. Die Bildinformationen der \textit{Kinect}-Kameras korrespondieren mit den im Umfeldmodell \ref{fig: consensenv} gezeigten Pfeilen. Eine der Gesichtserkennung vorangegangene Personenerkennung spart Rechenleistung ein. Dies wird in Kapitel \ref{sec: Funktionsweise des Gesamtsystems} genauer erläutert.\\
		
		\input{Bilder/wirkpers}
	
		
		In den Bildinformationen der integrierten \textit{Kinect}-Kameras befinden sich unter Anderem auch Tiefeninformationen passend zum gelieferten Farbbild. Somit wird die Distanz und die daraus resultierende Position einer Person ebenfalls errechnet. Das Grundlagenkapitel \ref{sec: Bestimmung der Positionskoordinaten} und der Abschnitt \ref{sec: Funktionsweise des Gesamtsystems} führt die Berechnung und die Methodik genauer aus. Eine Datenverarbeitung sorgt letztendlich für das Abspeichern möglichst vieler Information einer erkannten Person. Zukünftige Projekte am ALF können diese Eigenschaften für weitere Entwicklung nutzen.\\   
		
		\subsection{Konzept der Bildverarbeitung}
		\label{subsec: Auswahl und Training der verwendeten neuronalen Netze}
		
		In Kapitel \ref{subsec: Objekterkennung durch neuronale Netze} wurde bereits erwähnt, dass state-of-the-art Lösung in den meisten Fällen auf künstliche, neuronale Netze zurückgreifen. Insbesondere werden Konvolutionsnetze bevorzugt, wenn es um eine Umsetzung einer Objekterkennung geht. Auch in dieser Masterarbeit liegt das Hauptaugenmerk auf CNNs. Jedoch werden die in Kapitel \ref{subsec: Objekterkennung durch alternative Verfahren} beschrieben Methoden weiterhin berücksichtigt, indem ihre Leistung mit der der Faltungsnetze verglichen wird. Im Folgenden werden Vergleiche zwischen den in Kapitel \ref{subsec: Objekterkennung durch neuronale Netze} genannten künstlichen neuronalen Netzen gezogen. Weiterhin wird eine Auswahl für die praktische Anwendung am ALF getroffen. \\
		
		
		
		\input{Bilder/cnnvergleich}	
		
		Tabelle \ref{fig: cnnvergleich} zeigt den Vergleich unterschiedlicher Netzarchitekturen. Hierbei ist zu beachten, dass die Angaben je nach Trainings- und Testdatensatz variieren können. Die nähere Auswahl der gezeigten CNNs \textit{VGG-16}, \textit{ResNet50}, \textit{InceptionV3} und \textit{MobileNet} wurde bereits in Kapitel \ref{sec: cnns} begründet. Die Gegenüberstellung der Parameteranzahl und des daraus resultiernden Speicherplatzes zeigt deutlich, dass die \textit{MobileNet}-Architektur hierbei die kleinsten Werte aufweist. Bei der Betrachtung der Genauigkeiten im Hinblick auf die Tiefe der Netze fällt auf, dass sich hierbei offensichtlich keinerlei Relation beschrieben lässt. \textit{InceptionV3} weist bei den Top-$x$ Fehlern die höchsten Werte auf und hat somit die höchste Genauigkeit.\\
		
		Im Bezug auf die entwickelte Wirkstruktur \ref{fig: consenspers} zielt der Anwendungsfall darauf hinaus Personen sicher und schnell zu erkennen. Die Aufgabe der reinen Personendetektion fordert jedoch keine sicherheitsrelevanten Eigenschaften des Netzes, sodass geringe Abstriche bei der Genauigkeit in Kauf genommen werden können. Die \textit{MobileNet}-Architektur zeigt das gesuchte Zusammenspiel aus geringem Speicherbedarf und verhältnismäßig hoher Genauigkeit. Durch den Einsatz von \textit{MobileNet} wird im Vergleich zu \textit{InceptionV3} eine Einsparung des Speicherplatzes um circa 82 \percent bei einem Verlust der Top-1 Genauigkeit um lediglich 10 \percent. Für eine zukünftige Anwendungen auf einem eingebettetem System eignet sich somit im Vergleich zu den anderen dargestellten Netzarchitekturen \textit{MobileNet} am besten. Sollten zukünftige Projekte den Einsatz eines \textit{Raspberry Pis} oder auch andere Mikrocomputer nicht in Betracht ziehen, zeigt \textit{InceptionV3} entsprechende Werte für die Implementierung im integrierten Computer des ALFs.\\
		
		\input{Bilder/mobilessd}
		
		Bereits in Kapitel \ref{sec: cnns} wurde die Kombination diverser Architetkuren zur Merkmalsextraktion mit entsprechenden Detektoren beschreiben. In Tabelle \ref{fig: mobilessdtab} werden derartige Verknüpfungen und ihre Eigenschaften gezeigt. Die Bezeichnung \textit{mean average precision} (mAP) beschreibt die durchschnittliche mittlere Genauigkeit und hat sich in der Praxis als Vergleichswert derartiger Architekturen durchgesetzt. FPS steht für \textit{frames per second} und besagt, wie viele Bilder pro Sekunde verarbeitet werden können. In der Fachsprache wird der Teil der Merkmalextraktion eines Netzes häufig auch als \textit{Backbone} bezeichnet. Die Tabelle zeigt die Architekturen \textit{Faster R-CNN} und \textit{SSD} mit jeweils einem \textit{VGG-16} Netz als \textit{Backbone} und das \textit{MobileNet-SSD} aus Zhangs Paper \cite{embedded}. Zhang wendet dort ebenfalls die \textit{MobileNet-SSD} Architektur auf einem eingebettetem System an und erreicht eine rechenzeit von 1.13 FPS. Wie bereits in Kapitel \ref{sec: cnns} beschrieben können am ALF maximal bis zu 60 Bilder in der Sekunde eingehen. Mit 7 FPS und einer geringeren Genauigkeit als der \text{SSD (VGG-16)} ist der Einsatz des \text{Faster R-CNN} ausgeschlossen. Die \textit{MobileNet-SSD} Architektur erreicht laut der präsentierten Benchmark nahezu 60 FPS und nimmt aufgrund des \textit{MobileNet Backbones} weniger Speicherplatz ein als das \textit{SSD (VGG-16)} Netz. \\ 
		
		Das Kapitel \ref{ch: Verifikation} wird sich mit der tatsächlich gemessenen Leistung behandelter Netze im Feld beschäftigen. Hierbei wird sowohl der Einsatz am ALF als auch auf einem \textit{Raspberry Pi} berücksichtigt. Die durch die wissenschaftlichen Paper erlangten Informationen zeigen jedoch, welche Netzarten und -architekturen hierfür in Betracht gezogen werden können.
		
		\subsection{Entwickeltes neuronales Netz}
		\label{seubsec: Entwickeltes neuronales Netz}
		
		Die in Kapitel \ref{subsec: Auswahl und Training der verwendeten neuronalen Netze} behandelten neuronalen Netze wurden auf dessen Leistungsfähigkeit passend zum Anwendungsfall am ALF untersucht. Mittlerweile gibt es auf Seiten wie zum Beispiel \textit{Tensorflow.org} oder \textit{Keras.io} viele Möglichkeiten zum Download unterschiedlicher Netze. Viele der angebotenen KNNs sind in je nach Anwendungsfall in diversen Kategorien eingeteilt. Unter Anderem finden sich dort auch KNNs zur Objekterkennung. Die meisten sind darauf ausgelegt multiple Objekte zu erkennen. Somit lassen sich beispielsweise mit einem Netz nicht nur Personen sondern auch Flugzeuge, Autos und viele weitere Gegenstände des Alltags erkennen. \\ 
		
		\input{Bilder/ownnet}
		
		Für den Einsatz am ALF ist die Erkennung anderer Objekte jedoch unerheblich. Somit gibt es unter Anderem zwei Möglichkeiten diesen Sachverhalt zu lösen. Für den ersten Lösungsansatz wird das entsprechende KNN unverändert eingesetzt. Ein Sortieralgorhitmus beschäftigt sich nach der Analyse des Bildes damit, nicht relevante Klassen auszusortieren. In Abbildung \ref{fig: ownnet} ist eine weitere Lösung dargestellt. Die Ausgabeschicht eines Netzes wird so verändert, dass lediglich ein Ausgangsneuron vorhanden ist. Dieser ist bekanntlich in der Lage auf eine Klasse trainiert zu werden. Die restlichen Klassen werden somit verworfen und das neuron wird ausschließlich auf die Klasse "Person" trainiert. Diese Vorgehensweise spart nicht nur die Einbindung eines Sortieralgorhitmus sondern auch Speicherplatz ein. Durch die Elimiation der unbedeutsamen Neuronen werden folglich auch Parameter gelöscht, die gewissen Speicher einnehmen. Somit wird auch eine schnellere Rechenzeit erwartet. 
		
	\section{Konzept und Aufbau des Zustandsautomaten}
	\label{sec: Umsetzung der Statemachine}
	Bisher wurden für den Aufruf der Fahrfunktionen in \textit{ROS} ausführbare Dateien aufgerufen \cite{Bachelorarbeit}. Hierbei mussten der Benutzer darauf achten, diese Dateien nicht im geringen Zeitabstand und vor allem in der richtigen Reihenfolge aufzurufen. Zu jedem Anwendungsfall gehörten entsprechende Dateien. Ein hierarchischer Zustandsautomat unterbindet die Probleme indem sich der Endzustand durch zuvor aufgerufene Zustände zusammensetzt. Der Aufruf verschiedner \textit{ROS}-Knoten in der korrekten Reihenfolge wird somit autark geregelt. Für die Steuerung des EAs wird eine Spracherkennung verwendet. Die Auswahl der Spracherkennung zur Steuerung des Zustandsautomats wird in Kapitel \ref{sec: Motivation} begründet. Die Entwicklung der Spracherkennung wird in der Masterarbeit von Hannes Dittmann erläutert. Die Auswahl des Zustandsautomats wird im folgendem Kapitel näher ausgeführt.\\	
	
	\subsection{Auslegung des Zustandsautomats}
	Die Problematik der Steuerung über Sprache ist die Extraktion der eigentlichen Aussage eines Satzes. In der Masterarbeit von Hannes Dittmann werden aufgrund dessen Sprachbefehle kategorisiert \cite{Dittmann}. Die KI ist in der Lage verschiedene Sätze einer für den Roboter relevanten Kategorie zuzuordnen. Diese werden als Eingabeparameter für den EA genutzt. Weiterhin wird zwischen einer manuellen und einer autonomen Fahraufgabe unterschieden und als zweiten Parameter für den Zustandsautomaten genutzt. Anhand der technischen Fähigkeiten des Roboters wurden die Kategorienamen so gewählt, dass alle möglichen Handlungen abgedeckt sind. Der Zustandsautomat wurde so entworfen, dass er trotz der umfangreichen Fahrfunktionen des Roboters mit möglichst wenig Zuständen arbeitet. Außerdem deckt der Automat teilweise die Richtlinien nach Level 4 der in Kapitel \ref{sec: Motivation} gezeigten Tabelle ... . Für die größtmögliche Effizienz hinsichtlich der Dimension und Funktionsweise des EAs wurde ein mathematisches Modell entworfen.\\
	
	\begin{equation}
	\vec{z}=\sum_{z_0}^{z_f} \left[ \begin{array}{r}
	k_0  \\
	k_{1}  \\
	...  \\
	k_8  \\
	\end{array}\right] \circ
	\left[ \begin{array}{r}
	b_0(z)  \\
	b_{1}(z)  \\
	...  \\
	b_8(z)  \\
	\end{array}\right]  \circ
	\left[ \begin{array}{r}
	1  \\
	1  \\
	m_{2}(z_f)  \\
	m_{3}(z_f)   \\
	1  \\
	m_{5}(z_f)  \\
	1   \\
	m_{7}(z_f)   \\
	m_8(z_f)  \\
	\end{array}\right]
	\text{ für }z_f\in[0;8] \text{ und }z_0=0 oder 1
	\label{eq: statemachine}
	\end{equation}\\
	
		Anhand der Gleichungen \ref{eq: statemachine} und \ref{eq: Binärfunktion} wird der mathematische Hintergrund des EAs erläutert. Der hierarchische Zustandsautomat ermöglicht einen finalen, sich aufbauenden Zustand $\vec{z}$. Dieser wird anhand der in Gleichung \ref{eq: statemachine} dargestellten Summe berechnet. Beginnend vom Anfangszustand $z_0$ wird jeder mögliche $z$ bis zum finalen Zustand $z_f$ addiert. Bei der Addition geht es hauptsächlich darum, dem Anwendungsfall entsprechende \textit{ROS}-Knoten aufzurufen. Zu einem Zustand fest zugehörige Knotengruppen werden in Gleichung \ref{eq: statemachine} mit $k$ bezeichnet. Dessen Index deutet auf die Zugehörigkeit des Knotens zum jeweiligen Zustand. Hierbei werden die möglichen Endzustände wie in Abbildung \ref{fig: uml} nummeriert.\\
		
		Die Knotengruppe $k_0$ gehört zu dem Zustand \textit{Stop} und leitet den risikominalen Zustand ein. Dieser wird erreicht, indem alle \textit{ROS}-Knoten inklusive des des \textit{ROS}-Netzwerks heruntergefahren werden. So kann sichergestellt werden, dass keine Nachrichten an die Motorsteuergeräte der vier verbauten Motoren gesendet werden. Der Zustand \textit{Stop} kann nur verlassen werden, indem ein sicherer Wechsel in den Folgezustand \textit{Warten} durch den Benutzer zugesichert wird. Die Abfrage erfolgt sowohl über die Tastatur, als auch über Sprache. Alle folgenden Zustände enthalten der jeweiligen Anwendung entsprechende Knottengruppen. Eine Auflistung aller Zustände in im Anhang ... aufgeführt.\\ 
	
	\begin{equation}
	b(z)=\left\{\begin{array}{ll} 1 \text{ für } z=n \\
	0 \text{ für }z\neq n\end{array}\right. 
	\label{eq: Binärfunktion}
	\end{equation}\\
		
		Gleichung \ref{eq: Binärfunktion} zeigt die verwendete Binärfunktion $b(z)$. Je nach Iterationsschritt de- und aktiviert die Funktion $b(z)$ rein binär die korrekte Zeile in Gleichung \ref{eq: statemachine}. Nicht aktive Zustände werden mit 0 multipliziert sowie aktive mit einer einfachen Verstärkung beaufschlagt. \\
		
		Die ebenfalls binären Vektorelemente $m_n(z_f)$ sind mathematisch von der Zusammensetzung des Endzustands abhängig. Dieser Sachverhalt wird beispielsweise mithilfe der manuellen Fahrfunktion erklärt. Diese wird automatisiert mit einer statischen Karte gestartet, in der sich der Roboter durch einen Partikelfilter selbst findet. Hat der Benutzer jedoch zuvor eine Lokalisierung mithilfe der \textit{SLAM}-Methode gefordert, kann dieselbe Fahraufgabe mit einer sich aufbauenden Karte vollzogen werden. Widerrum schließen sich diverse Zustände gegenseitig aus. Eine bestimmte Zielpose ist von dem Ursprung einer statischen Karte abhängig. Diese ist beim \textit{SLAM}-Algorhitmus jeodch nicht vorhanden. So kann folglich kein Ziel angefahren werden, wenn zuvor die Lokalisierung mithilfe von \textit{SLAM} abgeschlossen wurde.\\
		
		\begin{figure}[H]
			\centering
			\begin{minipage}[b]{0.4\textwidth}
				\begin{equation}
					m_2(z_f)=\left\{\begin{array}{ll} 1 \text{ für } z_f=2,7 \\
					0 \text{ für }z_f\neq 2,7\end{array}\right. 
					\label{eq: m2}
				\end{equation}\\
			\end{minipage}
			\hfill
			\begin{minipage}[b]{0.55\textwidth}
				\begin{equation}
					m_3(z_f)=\left\{\begin{array}{ll} 1 \text{ für } z_f\neq2,7 \wedge m_2(z_f) = 0 \\
					0 \text{ für }z_f= 2,7\end{array}\right.
					\label{eq: m3}
				\end{equation}\\
			\end{minipage}
		\end{figure}
		
		Nur einer der Vektorelemente $m_2(z_f)$ und $m_3(z_f)$ kann aktiv sein. Der Modus entspricht auch hier dem Index und sagt aus, ob die Fahraufgabe mit einer statischen oder einer sich aufbauenden Karte bearbeitet werden soll. Hierfür gelten die bereits angesprochenen Restriktionen. Die drei fahrfähigen Endzustände \textit{Manuell}, \textit{Erkunden} und \textit{Ziel} werden durch $m_5$, $m_7$ und $m_8$ aktiviert. Der Zustandsautomat besitzt die Fähigkeit diverse Zwischenzustände als temporäre Endzustände $z_f$ auszuführen. Somit kann sich das ALF beispielsweise im Modus \textit{Statische Karte} $z_f=3$ ohne eine bestimmte Fahraufgabe selbst lokalisieren. In einem weiteren Schritt ist es dem Benutzer erlaubt vollständige und fahrfähige Endzustände auszuwählen wie zum Beispiel den Modus \textit{Ziel} $z_f=8$.\\
		
		
				\begin{equation}
					m_5(z_f)=\left\{\begin{array}{ll} 1 \text{ für } z_f=5 \\
						0 \text{ für }z_f\neq 5\end{array}\right. 
					\label{eq: m5}
				\end{equation}\\
			
		Auch bei den genannten, fahrfähigen Endzuständen gibt es diverse Einschränkungen. So kann der Manuelle Fahrmodus nur ausgewählt werden, wenn er als Endzustand $z_f$ definiert wurde. Die entsprechende Logik ist in Gleichung \ref{eq: m5} gezeigt.\\
		
				\begin{equation}
					m_7(z_f)=\left\{\begin{array}{ll} 1 \text{ für } z_f=7 \wedge m_2(z_f)=1 \wedge m_5(z_f)=0 \\
						0 \text{ für }z_f\neq 7\end{array}\right.
					\label{eq: m7}
				\end{equation}\\
				
		Gleichung \ref{eq: m7} stellt die Restriktionen des Zustands \textit{Erkunden} dar. Wenn der Modus als Endzustand gewählt wurde, die Lokalisierung ausschließlich durch die \textit{SLAM}-Methode realisiert wird und zuvor kein fahrfähiger Endzustand aktiviert wurde, darf der Zustand \textit{Erkunden} in Kraft treten. Für den Zustand \textit{Ziel} gilt grundlegend diesselbe Logik für die Aktivierung. Jedoch verlangt der Fahrmodus eine statische Karte. In Gleichung \ref{eq: m8} sind die Abhängigkeiten genauer verdeutlicht.\\
		
		
				\begin{equation}
					m_8(z_f)=\left\{\begin{array}{ll} 1 \text{ für } z_f=8 \wedge m_3(z_f)=1 \wedge m_5(z_f)=0 \wedge m_7(z_f)=0 \\
						0 \text{ für }z_f\neq 8\end{array}\right.
					\label{eq: m8}
				\end{equation}\\
		
		Für eine übersichtliche Darstellung eines Zustandsautomats eignet sich ein \textit{Unified Modeling Language} (UML) Diagramm. Das zum entworfenen Zustandsautomaten entsprechende UML-Diagramm ist in Abbildung \ref{fig: uml} gezeigt. Zustände sind als Boxen mit Abrundungen dargestellt und durch den Namen sowie den dazugehörigegen Index kennzeichnet. Pfeile stehen für Transitionen zwischen den Zuständen. Je nach Hierachieebene legen schwarze Punkte den Startpunkt fest, der durchlaufen werden muss. 
		
	
	
	
	\input{Bilder/uml} 
	
	\section{Funktionsweise des Gesamtsystems}
	\label{sec: Funktionsweise des Gesamtsystems}
	
	warum erst personenerkenung dan gesicht-- Unter Anderem wird die extrahierte \textit{Feature Map} des Gesichts, die Bildkoordinaten der Person und die tatsächliche globale Position einer Person abgespeichert.
	
	Der Kern dieser Arbeit ist die Personenerkennung im praktischen Kontext des im Kapitel \ref{ch: Einleitung} beschriebenen autonomen Logistikfahrzeugs. In Kapitel \ref{sec: Anforderungserhebung} wurden bereits alle Schnittstellen zu verbauten Hardware- und Softwarekomponenten präsentiert. Das vollständige System der Personenerkennung und der Aufbau des entwickelten Zustandautomats wird in diesem Kapitel erklärt.\\
	
	Wie bereits in Kapitel \ref{ch: Einleitung} beschrieben, wird die Personenerkennung am ALF mithilfe der Bildinformationen von zwei \textit{Kinect}-Kameras betrieben. Als Programmiersprache wird im Zuge dieser Masterarbeit \textit{Python} verwendet. Es bietet eine großes Spektrum an Softwarelösungen, wenn um das Thema Bildverarbeitung geht. Viele bekannte Frameworks wie \textit{Tensorflow} oder \textit{Keras} unterstützen \textit{Python}.\\
	
	Wie auch im Projekt der Bachelorarbeit, liefert das integrierte \textit{ROS}-Netzwerk die Bildinformation der Kameras. Die Schnittstelle zwischen \textit{ROS} und \textit{Python} bietet die Möglichkeit eingehende Bilder sowohl parallel, als auch seriell zu bearbeiten. Jedoch ist zu beachten, dass die Recheneinheit eines Computers durchaus mit der Bildverarbeitung eines Bildes mit bis zu über 50 \percent Rechenkapazität belastet sein kann. Demzufolge ist auf einem eingebettetem System eine deutlich höhere Belastung zu erwarten. Aufgrunddessen wird bei der Bildverarbeitung der Personenerkennung auf eine serielle Bearbeitung gesetzt.  Hierbei werden Pausen mit der gewünschten Dauer zwischen Bildverarbeitungsprozessen eingelegt bis ein relevantes Bild erkannt wird. Erst dann arbeitet die Personenerkennung mit der maximalen Geschwindigkeit. Als relevant werden Bilder eingestuft, die eine Person enthalten.\\
	
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}
			\node[circle,fill,gray,minimum size=15mm] (head) {};
			\node[rounded corners=2pt,minimum height=5cm,gray,minimum width=1.5cm,fill,below = 1pt of head] (body) {};
			\draw[line width=3.8mm,gray,round cap-round cap] ([shift={(6pt,-1pt)}]body.north east) --++(-90:19mm);
			\draw[line width=3.8mm,gray,round cap-round cap] ([shift={(-6pt,-1pt)}]body.north west)--++(-90:19mm);
			\draw[thick,white,-round cap] (body.south) --++(90:18mm);
			\node at (0,-2.5) [thick,dashed,rectangle,minimum height=7cm,minimum width=4cm,draw] (v100) {};
			\node at (0,-0.6) [thick,dashdotted,rectangle,minimum height=3cm,minimum width=3.8cm,draw] (v100) {};
			\node at (0,-0.0) [thick,rectangle,minimum height=1.6cm,minimum width=1.6cm,draw] (v100) {};
			
		\end{tikzpicture}
		\caption{test}
		\label{fig: bbox}
	\end{figure}

	In Abbildung \ref{fig: bbox} wird der Ablauf der Personenerkennung in Form eines Programmablaufplans dargestellt. Die Darstellung zeigt die Funktionsweise des Programms ab dem Zeitpunkt, an dem eine Person vor der jeweiligen Kamera detektiert wird. Weiterhin zeigt die Abbildung den Informationsfluss eines Bildes von einer Kamera durch die Personenerkennung. Zu Beginn der Analyse gelangt jedes Bild zunächst in das eingestellte künstliche neuronale Netz. Je nach Anzahl der erkannten Person werden korrespondierende Koordinaten ausgegeben, die die Position des Interessensbereich beschreiben. Anhand dieser Informationen kann an eine Aussage darüber getroffen werden, ob eine Person im Bild zu sehen ist und wo sich diese befindet. Wird keine Person erkannt arbeitet das Programm wieder reduziert, wie bereits beschrieben. Sollten jedoch Personen erkannt worden sein, wird versucht ein Gesicht zu erkennen. In den meisten Fällen sitzen und stehen Menschen aufrecht. So kann davon ausgegangen werden, dass sich das Gesicht einer Person im oberen Teil des Bereichs befindet. Dafür wird der Interessensbereich verkleinert, um Rechenkapazitäten einzusparen. Sollte sich kein Gesicht im relevanten Bereich befinden, wird davon ausgegangen, dass die Person stark von der Kamera abgewandt ist. Somit ist keine eindeutige Identifikation möglich und das Programm schaltet in den reduzierten Modus. Detektiert das Netz ein Gesicht wird eine Merkmalsextraktion durchgeführt.\\
	
	Im Anschluss werden die extrahierten Merkmale mit den der bereits abgespeicherten Gesichtern verglichen. Wird kein übereinstimmendes Gesicht gefunden versucht die Software das Gesicht zu registrieren. Je nach Einstellung wird ein Gesicht registriert, wenn es entsprechend oft hintereinander erkannt worden ist. Der Registrierungsprozess und die Sammlung von Bildinformationen wird in Kapitel \ref{subsec: Erstellung von Objektinformationen} behandelt. Im Falle der Erkennung eines bekannten Gesichts wird die Informationen der erkannten Person wie im bereits erwähnten Kapitel aktualisiert.\\ 
	
	  
	\newpage
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[node distance = 2cm, auto]
			
			% Place nodes
			\node [papStart] (Start1){Start};
			\node [papProcess, below of = Start1,label={[shift={(5,-0.6)}]\footnotesize\textit{Aktuelles Bild wird mit KNN analysiert}}] (pro1){Prozess};
			\node [papDecision, below of = pro1, yshift= -9mm,label={[shift={(2.7,-0.6)}]\footnotesize\textit{Menschen im Bild?}}](dec1){Entscheidung};
			\node [papProcess, right of = dec1,xshift=25mm,label={[shift={(5,-0.6)}]\footnotesize\textit{Gesichter im Interessensbereich detektieren}}](pro3){Prozess};
			\node [papDecision, below of = pro3, yshift= -9mm,label={[shift={(5,-0.6)}]\footnotesize\textit{Gesicht im Interessenbereich?}}](dec2){Entscheidung};
			\node [papProcess, below of = dec2, yshift= -9mm,label={[shift={(5,-0.6)}]\footnotesize\textit{Merkmalsextraktion des Gesichts}}](pro4){Prozess};
			\node [papDecision, below of = pro4, yshift= -9mm,label={[shift={(5,-0.6)}]\footnotesize\textit{Gesicht bekannt?}}](dec3){Entscheidung};
			\node [papDecision, below of = dec3, yshift= -18mm,label={[shift={(5,-0.6)}]\footnotesize\textit{Dasselbe unbekannte Gesicht oft genug hintereinander erkannt?}}](dec4){Entscheidung};
			\node [papProcess, below of = dec4, yshift= -9mm,label={[shift={(5,-0.6)}]\footnotesize\textit{Eigenschaften des Objekts vom Typ Mensch aktualisieren}}](pro5){Prozess};
			%\node [papData, right of = dec3, xshift= 25mm,label={[shift={(5,-0.6)}]\footnotesize\textit{Gesicht bekannt?}}](dat1){I/O};
			\node [papEnd, below of = dec1, yshift= -40mm] (End) {Ende};
			
			% Place joins
			\coordinate [below of = dec1, yshift= -9mm] (join1);
			
			% Draw edges
			\path [papLine] (Start1) -- (pro1);
			\path [papLine] (pro1) -- (dec1);
			\path [papLine] (dec1) -- node [above] {\papYes} (pro3);
			\draw (dec1) -- node [right] {\papNo} (join1);
			\path [papLine] (pro3) -- (dec2);
		%	\path [papLine] (dec2) -- node [above] {\papYes} (pro4)
			\draw (dec2) -- node [above] {\papNo} (join1);
			
			\path [papLine] (join1) -- (End);
			
		\end{tikzpicture}
		\label{fig: Personenerkennung}
	\end{figure}
	

		
	
		
		
				   		

