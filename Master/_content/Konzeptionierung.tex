\chapter{Konzeptionierung}
\label{ch: Konzeptionierung}

In diesem Kapitel wird die Konzeptionierung der Personenerkennung sowie die des Zustandsautomaten für das ALF erläutert. Wie auch in der vorangegangenen Bachelorarbeit wird die Anforderungserhebung mit der \textit{Conceptual design specification technique for the engineering of complex Systems} (CONSENS)-Methode durchgeführt \cite{Bachelorarbeit}. Die dadurch erhobenen Anforderungen an die beschriebenen Teilsysteme wurden im Lastenheft \ref{it: Lastenheft} als Anforderungsliste festgehalten.
	
	
	\section{Anforderungserhebung mit CONSENS}
	\label{sec: Anforderungserhebung}
	
	In Abbildung \ref{fig: consensenv} ist das erweiterte Umfeldmodell des ALFs zu sehen. Das ursprüngliche Umfeldmodell ist in der entsprechenden Bachelorarbeit wiederzufinden \cite{Bachelorarbeit}. Der Aufbau des Entwurfs besteht im Allgemeinen aus hellblauen und gelben Hexagonen, die Wirk- und Umfeldelemente repräsentieren. Der Informationsfluss zwischen den Elementen ist durch gestrichelte Pfeile gekennzeichnet.\\
	
	Das ALF gilt als Kern des Gesamtsystems und ist deswegen im Modell mittig dargestellt. Es interagiert sowohl mit Wirk- als auch mit Umfeldelementen, wie in Abbildung \ref{fig: consensenv} gezeigt. Zur Umsetzung von Fahraufgaben wird zusätzlich zur Personenerkennung der im Abschnitt \ref{sec: Zustandautomat} erwähnte Zustandsautomat entwickelt. Dieser ist in Abbildung \ref{fig: consensenv} mit einem gelben Hexagon dargestellt. Der EA wird mittels der von Herrn Dittmann entwickelten Sprachverarbeitung gesteuert. Diese kann, wie in Abbildung \ref{fig: consensenv} gezeigt, sowohl auf dem Rechner des ALFs als auch auf dem integrierten \textit{Raspberry Pi} ausgeführt werden \cite{Dittmann}. \\
	
	\input{Bilder/umfeldmodell}	
	
	Die Erweiterung um die Personenerkennung und Sprachverarbeitung ist in der in Abbildung \ref{fig: consensctrl} gezeigten Wirkstruktur dargestellt. Eine Wirkstruktur repräsentiert den Inhalt eines Wirkelements. In diesem Fall wird das Wirkelement mit dem Titel ALF aus dem vorangegangenem Umfeldmodell \ref{fig: consensenv} aufgeschlüsselt. Elemente mit blassen Farben sind für diese Masterarbeit nicht relevant und werden im Folgenden nicht behandelt.\\
	
	Für den Betrieb der Personenerkennung sind die Bildinformationen der integrierten \textit{Kinect}-Kameras notwendig. Als Ausgabe werden die Positionen von erkannten Personen veröffentlicht. Dies kann in zukünftigen Projekten beispielsweise als Einfluss auf eine Navigationsanwendung zur Vermeidung von Personenkontakt verwendet werden.\\
	
	\input{Bilder/wirknav}
	
	Für eine Interaktion mit anwesenden Personen werden Statusinformationen ausgegeben, wie oben links in Abbildung \ref{fig: consensctrl} dargestellt. Im Umfeldmodell wird die Klassifikation als Transitionsbedingung interpretiert und ist dort als solche gekennzeichnet. Somit hat die Ausgabe der Sprachverarbeitung durch die Klassifikation der Sprache einen Einfluss auf den Zustandsautomaten. In diesem Kapitel wird genauer auf die Konzeptionierung des Automaten eingegangen.\\
		

	
	
	
	\section{Konzept und Aufbau der Personenerkennung}
	\label{sec: Konzept Personenerkennung}
	
	Die Personenerkennung gilt als eine von zwei in dieser Masterarbeit entwickelten Erweiterungen und gleichzeitig als Hauptthema. Ziel hierbei ist die eindeutige Unterscheidung und Wiedererkennung von Personen durch das System. In diesem Abschnitt wird die Personenerkennung in ihrer Struktur und Umsetzung näher erläutert. Weiterhin werden konkurrierende Softwarekomponenten gegenübergestellt.\\
	
	Grundlage für eine Personenerkennung ist die eindeutige Identifikation eines Menschen. Folglich muss ein System in der Lage sein, äußerliche Merkmale festzustellen, die langfristig wiederzuerkennen sind. Aufgrund dessen musste das äußere Erscheinungsbild der Person, wie zum Beispiel die Kleidung oder die Körperhaltung dieser als Identifikationsmerkmal ausgeschlossen werden. Das Gesicht eines Menschen verändert sich mit fortlaufendem Alter. Es wird angenommen, dass sich die Benutzergruppe des ALFs auf Studenten und Mitarbeiter des Hochschule Bochum beschränkt. Mit dieser Annahme besteht die Benutzergruppe des Fahrzeugs aus Personen mit einem Alter zwischen 20 und 65 Jahren. Durch diese Beschränkung könnten individuelle Personen schätzungsweise mindestens 5 Jahre durch das Gesicht erkannt werden. Die eindeutige Erkennung eines Menschen setzt also voraus, dass das Gesicht einer Person von der Kamera des ALFs erfasst wird. \\ 
	
	\subsection{Wirkstruktur der Personenerkennung}
	\label{subsec: Wirkstrukur Personenerkennung}
	In Abbildung \ref{fig: consenspers} ist die Wirkstruktur der Personenerkennung dargestellt. Eine der Gesichtserkennung vorangegangene Personendetektion gibt die Bildkoordinaten der erkannten Menschen an die Positionsbestimmung und die Gesichtserkennung weiter. In den Bildinformationen der \textit{Kinect}-Kameras befinden sich unter anderem auch Tiefeninformationen passend zum gelieferten Farbbild. Somit wird die Distanz und die daraus resultierende Position einer Person ebenfalls errechnet. Die Abschnitte \ref{sec: Bestimmung der Positionskoordinaten} und \ref{sec: Funktionsweise des Gesamtsystems} führen die Berechnung und die Methodik genauer aus.\\
		
	\input{Bilder/wirkpers}
	

		
	Eine Datenverarbeitung sorgt letztendlich für das Abspeichern verwertbarer Informationen einer erkannten Person. Zukünftige Projekte am ALF können diese Eigenschaften für weitere Entwicklungen nutzen.\\   
		
	\subsection{Konzept der Bildverarbeitung}
	\label{subsec: Auswahl und Training der verwendeten neuronalen Netze}
		
	In Abschnitt \ref{subsec: Objekterkennung durch neuronale Netze} wurde bereits erwähnt, dass state-of-the-art Lösungen häufig auf künstliche, neuronale Netze zurückgreifen. Insbesondere werden Konvolutionsnetze bevorzugt, wenn es um eine Umsetzung einer Objekterkennung geht. Auch in dieser Masterarbeit werden CNNs untersucht. Jedoch werden die in Abschnitt \ref{subsec: Objekterkennung durch alternative Verfahren} beschrieben Methoden weiterhin berücksichtigt, indem ihre Leistung mit der der Faltungsnetze verglichen wird. Im Folgenden werden Vergleiche zwischen den in Abschnitt \ref{subsec: Objekterkennung durch neuronale Netze} genannten künstlichen neuronalen Netzen gezogen. Weiterhin wird eine Auswahl für die praktische Anwendung am ALF getroffen. \\
		
		
		
		\input{Bilder/cnnvergleich}	
		
		Tabelle \ref{fig: cnnvergleich} zeigt den Vergleich unterschiedlicher Netzarchitekturen. Hierbei ist zu beachten, dass die Angaben je nach Trainings- und Testdatensatz variieren können. Die nähere Auswahl der gezeigten CNNs \textit{VGG-16}, \textit{ResNet50}, \textit{InceptionV3} und \textit{MobileNet} wurde bereits in Abschnitt \ref{sec: cnns} begründet. Die Gegenüberstellung der Parameteranzahl und des daraus resultierenden Speicherplatzes zeigt deutlich, dass die \textit{MobileNet}-Architektur hierbei die kleinsten Werte aufweist. Bei der Betrachtung der Genauigkeiten im Hinblick auf die Tiefe der Netze fällt auf, dass sich offensichtlich keinerlei Relation beschreiben lässt. \textit{InceptionV3} weist bei den Top-$x$ Fehlern die höchsten Werte auf und hat somit die höchste Genauigkeit.\\
		
		Im Bezug auf die entwickelte Wirkstruktur \ref{fig: consenspers} zielt der Anwendungsfall darauf ab, Personen sicher und schnell zu erkennen. Die Aufgabe der reinen Personendetektion erfordert jedoch keine sicherheitsrelevanten Eigenschaften des Netzes, sodass geringe Abstriche bei der Genauigkeit in Kauf genommen werden können. Die \textit{MobileNet}-Architektur zeigt das gesuchte Zusammenspiel aus geringem Speicherbedarf und verhältnismäßig hoher Genauigkeit. Durch den Einsatz von \textit{MobileNet} wird im Vergleich zu \textit{InceptionV3} eine Einsparung des Speicherplatzes um circa 82 \% bei einem Verlust der Top-1 Genauigkeit um lediglich 10 \% erreicht. Für eine zukünftige Anwendung auf einem eingebetteten System eignet sich die Umsetzung mithilfe von \textit{MobileNet}.\\
		
		\input{Bilder/mobilessd}
		
		Bereits in Abschnitt \ref{sec: cnns} wurde die Kombination diverser Architekturen zur Merkmalsextraktion mit entsprechenden Detektoren beschrieben. In Tabelle \ref{fig: mobilessdtab} werden derartige Verknüpfungen und ihre Eigenschaften gezeigt. In der Fachsprache wird der Teil der Merkmalsextraktion eines Netzes häufig auch als \textit{Backbone} bezeichnet. Die Tabelle zeigt die Architekturen \textit{Faster R-CNN} und \textit{SSD} mit jeweils einem \textit{VGG-16} Netz als \textit{Backbone} und das \textit{MobileNet SSD} aus Zhangs Publikation \cite{embedded}. Zhang wendet dort ebenfalls die \textit{MobileNet-SSD} Architektur auf einem eingebetteten System an und erreicht eine Rechenzeit von 1.13 FPS. Wie bereits in Abschnitt \ref{sec: cnns} beschrieben können am ALF maximal bis zu 60 Bilder in der Sekunde eingehen. Mit 7 FPS und einer geringeren Genauigkeit als der \text{SSD (VGG-16)} ist der Einsatz des \text{Faster R-CNN} ausgeschlossen. Die \textit{MobileNet-SSD} Architektur erreicht laut der präsentierten Benchmark nahezu 60 FPS und nimmt aufgrund des \textit{MobileNet Backbones} weniger Speicherplatz ein als das \textit{SSD (VGG-16)} Netz \cite{leightweight}. \\ 
		
		Das Kapitel \ref{ch: Verifikation} wird sich mit der tatsächlich gemessenen Leistung der behandelten Netze im Feld beschäftigen. Hierbei wird der Einsatz auf verschiedenen Hardwareplattformen getestet. Die durch die wissenschaftlichen Beiträge erlangten Informationen zeigen jedoch, welche Netzarten und -architekturen hierfür in Betracht gezogen werden können. Über die \textit{SSDLite}-Struktur gibt es keine aussagekräftigen Benchmarks. Auch diese Architektur wird in der Evaluation in Kapitel \ref{ch: Verifikation} berücksichtigt.
		
		\subsection{Entwicklung eines neuronalen Netzes}
		\label{subsec: Entwickeltes neuronales Netz}
		
		Die in Abschnitt \ref{subsec: Auswahl und Training der verwendeten neuronalen Netze} behandelten neuronalen Netze wurden auf ihre Leistungsfähigkeit passend zum Anwendungsfall am ALF untersucht. Weiterhin kann die Struktur eines gewählten KNNs optimiert werden. Der Vorgang wird in diesem Abschnitt beschrieben.\\
		
		Mittlerweile gibt es auf Seiten wie zum Beispiel \textit{Tensorflow.org} oder \textit{Keras.io} viele Möglichkeiten zum Download vortrainierter Netze. Viele der angebotenen KNNs sind je nach Anwendungsfall diversen Kategorien zugeordnet. Unter anderem finden sich dort auch KNNs zur Objekterkennung. Die meisten sind darauf ausgelegt, multiple Objekte zu erkennen. Somit lassen sich beispielsweise mit einem Netz nicht nur Personen, sondern auch Flugzeuge, Autos und viele weitere Gegenstände des Alltags erkennen. \\ 
		
		\input{Bilder/ownnet}
		\newpage
		Für den Einsatz am ALF ist die Erkennung anderer Objekte jedoch unerheblich. Zwei Lösungsansätze werden im Rahmen dieser Masterthesis genauer betrachtet. Für den ersten Lösungsansatz wird das entsprechende KNN unverändert eingesetzt. Ein Sortieralgorithmus beschäftigt sich nach der Analyse des Bildes mit dem Ausschluss nicht relevanter Klassen.\\
		
		In Abbildung \ref{fig: ownnet} ist eine weitere Lösung dargestellt. Die Ausgabeschicht eines Netzes wird so verändert, dass lediglich ein Ausgangsneuron vorhanden ist. Dieses ist bekanntlich in der Lage, auf eine Klasse trainiert zu werden. Die restlichen Klassen werden somit verworfen und das Neuron wird ausschließlich auf die Klasse \textit{Person} trainiert. Hierbei sei zu beachten, dass diese Veränderung nicht mit einer \textit{Softmax} Ausgabeschicht zu realisieren ist, da das Ausgangsneuron in jedem Fall die Wertigkeit eins haben würde. Der technische Hintergrund ist in Abschnitt \ref{subsec: Objekterkennung durch neuronale Netze} beschrieben.\\
		
		Diese Vorgehensweise spart nicht nur die Einbindung eines Sortieralgorithmus, sondern auch Speicherplatz ein. Durch die Elimination der unbedeutsamen Neuronen werden folglich auch Parameter gelöscht, die gewissen Speicher einnehmen und Rechenoperationen vorraussetzen. Somit wird auch eine schnellere Rechenzeit erwartet. \\
		
		Zur Umsetzung des KNNs wird \textit{Tensorflow} der Firma \textit{Google} verwendet. \textit{Tensorflow} bietet neben der Möglichkeit des Trainings vortrainierter Netze, Lösungen zur Implementierung auf eingebetteten Systemen \cite{frameworks}. Diesbezüglich haben die Entwickler einen eigenen Framework namens \textit{Tensorflow Lite} ins Leben gerufen. Dieser Framework ist insbesondere für die Entwicklung mobiler Applikationen ausgelegt \cite{tflite}.\\
		
		\textit{Tensorflow Lite} bietet beispielsweise die Möglichkeit, Netze einer sogenannten Quantisierung zu unterziehen \cite{tflite}. Dies ermöglicht einen noch kleineren FPS-Wert bei geringer Einsparung der Genauigkeit.\\
		
		\textit{Xus} Publikation zeigt, dass \textit{Tensorflow} und \textit{Tensorflow Lite} die meistgenutzten Frameworks sind, wenn es um mobile Applikationen geht. Weiterhin unterstützt \textit{Tensorflow} die \textit{NVIDIA CUDA Deep Neural Network} (cuDNN) Bibliothek \cite{frameworks}. \textit{NVIDIA cuDNN} ist für Grafikprozessoren und die Arbeit mit künstlichen neuronalen Netzen optimiert \cite{frameworks}. Außerdem unterstützt \textit{Tensorflow} die Betriebssysteme \textit{Linux}, \textit{Mac OS X} und \textit{Windows} \cite{frameworks}. Somit kann der integrierte Computer des autonomen Logistikfahrzeugs als ausführende Instanz verwendet werden.\\
		
		Für Trainingsprozesse eignet sich in vielen Fällen ein Rechner mit einer leistungsstarken Grafikeinheit. Die Praxiserfahrung dieser Masterarbeit zeigte, dass eine Grafikkarte im Vergleich zu einer Recheneinheit den Trainingsprozess der CNNs um den Faktor 12 beschleunigt. Jedoch gibt es Anwendungsfälle, die durch die Verwendung der zentralen Recheneinheit des Computers beschleunigt werden \cite{cpugpu}. \\
		
		Als Datensatz für das Training der Netze wird der \textit{Common Objects in Context} (COCO) Datensatz der Firma \textit{Microsoft} verwendet. Die dem Datensatz beigefügte \textit{Application Programming Interface} (API) ermöglichst es dem Benutzer, Datenmengen anhand der gewünschten Klassen zu extrahieren.\\
		
		Der Datensatz enthält circa 330000 Bilder mit diversen alltäglichen Objekten \cite{coco, cocopaper}. Davon sind laut eigener Aussagen über 200000 Bilder mit sogenannten \textit{Annotations} oder auch \textit{Labels} (deutsch: Anmerkungen/Etiketten) versehen \cite{coco}. Die Daten sind in Trainings-, Evaluations- und Testdaten unterteilt. Letzteres ist jedoch nicht etikettiert und somit für diese Masterarbeit nicht relevant.\\
		
		Die Trainingsdaten von dem \textit{COCO}-Datensatz aus dem Jahr 2017 stellt knapp 65000 etikettierte Bilder von Personen bereit. Circa 5000 Bilder sind im Evalutaionsdatensatz enthalten. Aufgrund des Umfangs und der entsprechenden API, die eine vereinfachte Extraktion der Daten ermöglicht, wird der \textit{COCO}-Datensatz verwendet. \\
		
		Für den Lernprozess wird der Trainingsdatensatz in Trainings- und Testdaten unterteilt. So kann ein Teil der eigentlichen Trainingsdaten durch die vorhandenen Etiketten als Testdatensatz verwendet werden. In Kapitel \ref{ch: Verifikation} werden bereits besprochene Netze mit dem eigenen und dem \textit{COCO}-Datensatz evaluiert.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=1\textwidth]{Bilder/coco1.jpg}
			\caption{Beispiel eines Bildes aus dem erstellten \textit{COCO}-Testdatensatz. Dargestellt ist eine Menschenmenge auf einer Grünfläche. Personen sind teilweise verdeckt.}
			\label{fig: cocotest}
		\end{figure}
	
		In Abbildung \ref{fig: cocotest} ist ein Bild des erstellten \textit{COCO}-Testdatensatzes gezeigt. Personen sind im Hintergrund teilweise verdeckt oder nicht eindeutig dargestellt. Dies erschwert die Erkennung der beschriebenen Personen und könnte somit das Ergebnis einer Evaluation negativ beeinflussen. \\ 
		
		Für einen zweiten Test wird ein eigener Datensatz erstellt. Hierbei werden 160 Bilder aus dem Einsatzumfeld des ALFs aufgenommen und etikettiert. Die Abbildungen \ref{fig: owndata}a und b zeigen zwei Beispiele des Datensatzes. 
		Im Rahmen dieser Masterarbeit wird aufgrund der in Kapitel \ref{ch: Einleitung} beschriebenen Anwendungen die Erkennung von Personen insbesondere auf den Nahbereich des Fahrzeugs ausgelegt. Aufgrund dessen sind Personen auf den Bildern des eigenen Datensatzes häufig verhältnismäßig groß dargestellt.\\
		
			\begin{figure}[H]
			\centering
			\begin{minipage}[b]{0.49\textwidth}
				(a)
				\includegraphics[width=0.94\textwidth]{Bilder/26.jpg}
			\end{minipage}
			\hfill
			\begin{minipage}[b]{0.49\textwidth}
				(b)
				\includegraphics[width=0.94\textwidth]{Bilder/39.jpg}
			\end{minipage}
			\caption{(a) Foto von zwei Personen im Labor für Antriebstechnik der Hochschule Bochum. (b) Darstellung von zwei Menschen. Eine Person ist teilweise verdeckt. }
			\label{fig: owndata}
		\end{figure}
		
		Trotzdem können Menschen auch am Einsatzort des ALFs durch verschiedene Objekte verdeckt werden. Abbildung \ref{fig: owndata}b zeigt den entsprechenden Anwendungsfall. Daher wurden derartige Situationen bei der Aufnahme des Datensatzes berücksichtigt und simuliert.
		
	
		
	\subsection{Funktionsweise der Personenerkennung}
	\label{sec: Funktionsweise des Gesamtsystems}
	
	Der Kern dieser Arbeit ist die Personenerkennung im praktischen Kontext des in Kapitel \ref{ch: Einleitung} beschriebenen autonomen Logistikfahrzeugs. In Abschnitt \ref{sec: Anforderungserhebung} wurden bereits alle Schnittstellen zu verbauten Hardware- und Softwarekomponenten präsentiert. Das vollständige System der Personenerkennung ist im Folgenden erklärt.\\
	
	Wie bereits in Kapitel \ref{ch: Einleitung} dieser Masterarbeit beschrieben, wird die Personenerkennung am ALF mithilfe der Bildinformationen von zwei \textit{Kinect}-Kameras betrieben. Als Programmiersprache wird im Zuge dieser Masterarbeit \textit{Python} verwendet. Mit Softwarepaketen wie beispielsweise \textit{OpenCV} oder \textit{Pillow} bietet \textit{Python} ein großes Spektrum an Softwarelösungen für die Bildverarbeitung. Bekannte Frameworks wie \textit{Tensorflow} oder \textit{Keras} unterstützen \textit{Python} \cite{tensorflow, keraspython}.\\
	
	Wie auch in dem Projekt der Bachelorarbeit liefert das integrierte \textit{ROS}-Netzwerk die Bildinformation der Kameras \cite{Bachelorarbeit}. Die Schnittstelle zwischen \textit{ROS} und \textit{Python} bietet die Möglichkeit, eingehende Bilder sowohl parallel als auch seriell zu bearbeiten. Jedoch ist zu beachten, dass die Recheneinheit eines Computers mit der Bildverarbeitung eines Bildes stark belastet sein kann. Der beschriebene Effekt variiert je nach verwendeter Hardwareplattform. Demzufolge ist auf einem eingebetteten System eine deutlich höhere Belastung zu erwarten als auf dem Computer des ALFs. Aufgrund dessen wird bei der Bildverarbeitung der Personenerkennung auf eine serielle Bearbeitung gesetzt. So wird ausgeschlossen, dass zwei eingehende Bilder gleichzeitig analysiert werden.\\
	

	Zu Beginn arbeitet das entwickelte System in einem reduzierten Modus. Hierbei werden Pausen mit einer gewünschten Dauer zwischen Bildverarbeitungsprozessen eingelegt, bis ein relevantes Bild erkannt wird. Als relevant werden Bilder eingestuft, die eine Person enthalten. Erst dann arbeitet die Personenerkennung mit der maximalen Geschwindigkeit. Der reduzierte Modus erspart weitere Rechenkapazität des Computers für parallel laufende Prozesse. 
		

	
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=1\textwidth]{Bilder/person.pdf}
		\caption{Konzept eines Fallbeispiels der Personenerkennung. Zu sehen sind zwei Personen im Labor für Antriebstechnik der Hochschule Bochum. Rote Rechtecke zeigen den Interessensbereich der Personendetektion. Die \textit{Bounding Box} der Gesichtsdetektion ist gestrichelt dargestellt. Das als Strichpunkt präsentierte Rechteck deutet den Bereich der Distanzmessung an.}
		\label{fig: bbox}
	\end{figure}
		In Abbildung \ref{fig: Personenerkennung} wird der Ablauf der Personenerkennung in Form eines Programmablaufplans dargestellt. Zu Beginn der Analyse gelangt jedes Bild in das verwendete künstliche neuronale Netz. Je nach Anzahl der erkannten Person werden korrespondierende \textit{Bounding Boxes} ausgegeben, die die Position des Interessensbereichs im Bild beschreiben. Dieser Vorgang ist beispielhaft in Darstellung \ref{fig: bbox} präsentiert.\\
		
		Die dort abgebildeten Personen werden in diesem Fall von roten \textit{Bounding Boxes} umrandet. Wird keine Person erkannt, verfällt das Programm wieder in den bereits beschriebenen, reduzierten Modus. Die extrahierten \textit{Bounding Boxes} gelten im Falle einer Personendetektion als Interessensbereich für die Gesichtserkennung.\\
		
			\begin{figure}[H]
			\centering
			\begin{tikzpicture}[node distance = 1.2cm, auto]
			
			% Place nodes
			\node [papStart] (Start1){Start};
			\node [papProcess, below of = Start1] (pro1){Prozess};
			\node [papDecision, below of = pro1, yshift= -9mm,label={[align=left,anchor=west,shift={(1.2,-0.3)}]\footnotesize\textit{Menschen im Bild?}}](dec1){Entscheidung};
			\node [papProcess, right of = dec1,xshift=25mm](pro3){Prozess};
			\node [papDecision, below of = pro3, yshift= -9mm](dec2){Entscheidung};
			\node [papProcess, below of = dec2, yshift= -9mm](pro4){Prozess};
			\node [papDecision, below of = pro4, yshift= -9mm,label={[align=left,anchor=west,shift={(1.2,-0.3)}]\footnotesize\textit{Gesicht bekannt?}}](dec3){Entscheidung};
			\node [papDecision, below of = dec3, yshift= -18mm,label={[align=left,anchor=west,shift={(1.2,-0.3)}]\footnotesize\textit{\makecell[l]{Registrierungs-\\prozess\\einleiten?}}}](dec4){Entscheidung};
			%\node [papData, right of = dec3, xshift= 25mm,label={[shift={(5,-0.6)}]\footnotesize\textit{Gesicht bekannt?}}](dat1){I/O};		
			\node [papProcess, below of = dec4,yshift= -9mm](pro6){Prozess};
			\node [papProcess, below of = pro6](pro5){Prozess};
			\node [papEnd, below of = pro5,xshift=-37mm] (End) {Ende};
			
			\node[right of = pro1,xshift=0.5cm,align=left,anchor=west] (P1) {\footnotesize\textit{\makecell[l]{Personendetektion\\durch KNN}}};
			\node[right of = pro3,xshift=0.5cm,align=left,anchor=west] (P2) {\footnotesize\textit{\makecell[l]{Gesichtsdetektion\\durch KNN}}};
			\node[right of = dec2,xshift=0.5cm,align=left,anchor=west] (D2) {\footnotesize\textit{\makecell[l]{Gesicht im\\Interessensbereich?}}};
			\node[right of = pro4,xshift=0.5cm,align=left,anchor=west] (P3) {\footnotesize\textit{\makecell[l]{Merkmalsextraktion\\des Gesichts}}};
			\node[right of = pro6,xshift=0.5cm,align=left,anchor=west] (P1) {\footnotesize\textit{\makecell[l]{Person\\registrieren}}};
			\node[right of = pro5,xshift=0.5cm,align=left,anchor=west] (P1) {\footnotesize\textit{\makecell[l]{Person\\aktualisieren}}};
			
			% Place joins
			\coordinate [below of = dec1, yshift= -9mm] (join1);
			\coordinate [right of = dec3,xshift=25mm] (join2);
			\coordinate [left of = dec4, xshift= -25mm] (join3);
			\coordinate [right of = pro5,xshift=25mm] (join4);
			\coordinate [left of = pro5,xshift=-25mm] (join5);
			
			% Draw edges
			\path [papLine] (Start1) -- (pro1);
			\path [papLine] (pro1) -- (dec1);
			\path [papLine] (dec1) -- node [above] {\papYes} (pro3);
			\draw (dec1) -- node [left] {\papNo} (join1);
			\path [papLine] (pro3) -- (dec2);
			%	\path [papLine] (dec2) -- node [above] {\papYes} (pro4)
			\draw (dec2) -- node [above] {\papNo} (join1);
			\path [papLine] (dec3) -- node [left] {\papNo} (dec4);
			\draw (dec3) -- node [above] {\papYes} (join2);
			\path [papLine] (join1) -- (End);
			\draw (dec4) -- node [above] {\papNo} (join3);
			\draw (join2) -- (join4);
			\path [papLine] (join4) -- (pro5);
			\draw (pro5) -- (join5);
			\path [papLine] (dec4) -- node [left] {\papYes} (pro6);
			\path [papLine] (pro6) -- (pro5);
			\path [papLine] (dec2) -- node [left] {\papYes} (pro4);
			\path [papLine] (pro4) -- (dec3);
			
			\end{tikzpicture}
			\caption{Prozessablaufplan der Personenerkennung. Der Start sowie das Ende des Programms sind als Boxen mit Abrundungen dargestellt. Rechtecke zeigen Prozesse und Parallelogramme deuten eine Entscheidung im Programmablauf an. Die Flussrichtung der entsprechenden Informationen werden durch Pfeile präsentiert.}
			\label{fig: Personenerkennung}
		\end{figure}
		\newpage
		
	Das Pythonpaket \textit{face-recognition} wird in diesem Projekt für die Gesichterkennung verwendet. Prinzipiell arbeitet das Paket in drei Schritten. Die in Abschnitt \ref{subsec: Objekterkennung durch alternative Verfahren} erklärte \textit{HOG}-Methode wird zur Extraktion der \textit{Bounding Box}, des jeweiligen Gesichts angewendet \cite{facerecarticle}.\\
	
	Damit eine Person nicht unbedingt gerade in das entsprechende Aufnahmegerät schauen muss, werden durch die Software sogenannte Landmarken auf dem Gesicht verteilt \cite{facerecarticle}. Diese werden zur Rotation des Gesichts im Bild verwendet \cite{facerecarticle}. \\
	
	Das bearbeitete Bild des Gesichts wird mit einem neuronalen Netz analysiert, das von den Entwicklern von \textit{OpenFace} bereitgestellt wird \cite{facerecarticle}. Entwickler \textit{Adam Geitgey} gibt für das im Paket verwendete Netz eine Genauigkeit von 99,38 \% an \cite{facerecognition}. Hinsichtlich der in Kapitel \ref{ch: Grundlagen} besprochenen, möglichen Nutzungsszenarien der Personenerkennung ist eine hohe Genauigkeit notwendig. \\
	
	Nach eigener Recherche besitzen die von \textit{OpenFace} bereitgestellten, neuronalen Netze zwischen 3,7 - 7,4 Millionen Parameter \cite{openface}. Die Größenordnung ähnelt folglich der einer herkömmlichen \textit{MobileNet}-Architektur.\\
	
	Außerdem bietet das \textit{face-recognition} Paket eine Funktion zum Vergleichen und Unterscheiden von Gesichtsmerkmalen. Aufgrund der Genauigkeit und der verwendeten Netze wird das \textit{face-recognition} Paket in dieser Masterarbeit für die Gesichtserkennung eingesetzt.\\
	
	Es kann davon ausgegangen werden, dass sich das Gesicht einer Person im oberen Teil eines extrahierten Begrenzungsrahmens befindet. Aufgrund dessen wird der Interessensbereich entsprechend verkleinert. Somit muss die Gesichtserkennung nicht den vollständigen Bereich durchsuchen und es werden weitere Rechenoperationen eingespart. Sollte sich kein Gesicht im relevanten Bereich befinden, wird davon ausgegangen, dass die Person stark von der Kamera abgewandt ist. Somit ist keine eindeutige Identifikation möglich und es erfolgt ein Neustart des Programms.\\
	
	Detektiert das Netz jedoch ein Gesicht, wird eine Merkmalsextraktion durchgeführt. Im Anschluss werden die extrahierten Merkmale mit denen der bereits abgespeicherten Gesichter verglichen. Wird kein übereinstimmendes Gesicht gefunden, leitet das System einen Registrierungsprozess ein. Je nach Einstellung der Häufigkeit wird ein Gesicht registriert, wenn es entsprechend oft hintereinander erkannt worden ist. Im Falle der Erkennung eines bekannten Gesichts werden Eigenschaften der erkannten Person, wie zum Beispiel die Gesichtsmerkmale oder die Position in der aktuellen Karte aktualisiert. \\ 
	
		
	\section{Konzept und Aufbau des Zustandsautomaten}
	\label{sec: Umsetzung der Statemachine}
	Bisher wurden für den Aufruf der Fahrfunktionen in \textit{ROS} ausführbare Dateien aufgerufen \cite{Bachelorarbeit}. Hierbei musste der Benutzer darauf achten, diese Dateien nicht im geringen Zeitabstand und vor allem in der richtigen Reihenfolge aufzurufen \cite{Bachelorarbeit}. Zu jedem Anwendungsfall gehören entsprechende Dateien. \\
	
	Ein hierarchischer Zustandsautomat nach Mealy unterbindet die Probleme, indem sich der Endzustand durch zuvor aufgerufene Zustände zusammensetzt. Der Aufruf verschiedner \textit{ROS}-Knoten in der korrekten Reihenfolge wird somit autark geregelt. Für die Steuerung des EAs wird die Spracherkennung aus der parallel laufenden Masterarbeit verwendet. Die Auswahl des Zustandsautomats wird im folgendem Abschnitt näher ausgeführt.\\	
	
	Die Problematik der Steuerung über Sprache ist die Extraktion der eigentlichen Aussage eines Satzes \cite{Dittmann}. In der Masterarbeit von \textit{Hannes Dittmann} werden Sprachbefehle kategorisiert \cite{Dittmann}. Die KI ist in der Lage verschiedene Sätze einer für den Roboter relevanten Kategorie zuzuordnen. Diese werden als Transitionsbedingung für den EA genutzt. Weiterhin wird zwischen einer manuellen und einer autonomen Fahraufgabe unterschieden. Die Information wird dann als zweite Bedingung für den Zustandsautomaten verwendet. Anhand der technischen Fähigkeiten des Roboters wurden die Kategorienamen so gewählt, dass alle möglichen Handlungen abgedeckt sind. Der Zustandsautomat wurde so entworfen, dass er trotz der umfangreichen Fahrfunktionen des Roboters mit möglichst wenig Zuständen arbeitet. Außerdem deckt der Automat die Richtlinien nach Level 5 der in Kapitel \ref{ch: Einleitung} gezeigten Tabelle ab. Für die größtmögliche Effizienz hinsichtlich der Dimension und Funktionsweise des EAs wurde ein mathematisches Modell entworfen.\\
	
	\begin{equation}
	\vec{\epsilon}=\sum_{\epsilon_0}^{\epsilon_f} \left[ \begin{array}{r}
	k_0  \\
	k_{1}  \\
	...  \\
	k_8  \\
	\end{array}\right] \circ
	\left[ \begin{array}{r}
	g_0(\epsilon)  \\
	g_{1}(\epsilon)  \\
	...  \\
	g_8(z)  \\
	\end{array}\right]  \circ
	\left[ \begin{array}{r}
	1  \\
	1  \\
	m_{2}(\epsilon_f)  \\
	m_{3}(\epsilon_f)   \\
	1  \\
	m_{5}(\epsilon_f)  \\
	1   \\
	m_{7}(\epsilon_f)   \\
	m_8(\epsilon_f)  \\
	\end{array}\right]
	\text{ für }\epsilon_f\in[0;8] \text{ und }\epsilon_0=0 \vee 1
	\label{eq: statemachine}
	\end{equation}\\
	
		Anhand der Gleichungen \ref{eq: statemachine} und \ref{eq: Binärfunktion} wird der mathematische Hintergrund des EAs erläutert. Der hierarchische Zustandsautomat ermöglicht einen finalen, sich aufbauenden Zustand $\vec{\epsilon}$. Dieser wird anhand der in Gleichung \ref{eq: statemachine} dargestellten Summe berechnet. Beginnend von dem Anfangszustand $\epsilon_0$ wird jeder mögliche $\epsilon$ bis zum finalen Zustand $\epsilon_f$ addiert. Bei der Addition geht es hauptsächlich darum, dem Anwendungsfall entsprechende \textit{ROS}-Knoten aufzurufen. Zu einem Zustand fest zugehörige Knotengruppen werden in Gleichung \ref{eq: statemachine} mit $k$ bezeichnet. Dessen Index deutet auf die Zugehörigkeit des Knotens zum jeweiligen Zustand an. Hierbei werden die möglichen Endzustände wie in Abbildung \ref{fig: uml} nummeriert.\\
		
		Die Knotengruppe $k_0$ gehört zu dem Zustand \textit{Stop} und leitet den risikominimalen Zustand ein. Dieser wird erreicht, indem alle \textit{ROS}-Knoten inklusive des \textit{ROS}-Netzwerks heruntergefahren werden. So kann sichergestellt werden, dass keine Nachrichten an die Motorsteuergeräte der vier verbauten Motoren gesendet werden. Der Zustand \textit{Stop} kann nur verlassen werden, indem ein sicherer Wechsel in den Folgezustand \textit{Warten} durch den Benutzer zugesichert wird. Die Abfrage erfolgt sowohl über die Tastatur als auch über Sprache. Alle folgenden Zustände enthalten der jeweiligen Anwendung entsprechende Knotengruppen. Eine Auflistung aller Zustände inklusive der Knotengruppen ist im Anhang \ref{Abbildungen} in der Abbildung \ref{fig: knotenmengen} aufgeführt.\\ 
	
	\begin{equation}
	g(\epsilon)=\left\{\begin{array}{ll} 1 \text{ für } \epsilon=n \\
	0 \text{ für }\epsilon\neq n\end{array}\right. 
	\label{eq: Binärfunktion}
	\end{equation}\\
		
		Gleichung \ref{eq: Binärfunktion} zeigt die verwendete Binärfunktion $g(\epsilon)$. Je nach Iterationsschritt de- und aktiviert die Funktion $g(\epsilon)$ rein binär die entsprechende Zeile in Gleichung \ref{eq: statemachine}. Nicht aktive Zustände werden mit 0 multipliziert. Aktive Zustände werden mit einer einfachen Verstärkung beaufschlagt. \\
		
		Die ebenfalls binären Vektorelemente $m_n(\epsilon_f)$ sind mathematisch von der Zusammensetzung des Endzustands abhängig. Dieser Sachverhalt wird beispielsweise mithilfe der manuellen Fahrfunktion erklärt. Diese wird automatisiert mit einer statischen Karte gestartet, in der sich der Roboter durch einen Partikelfilter selbst findet. Hat der Benutzer jedoch zuvor eine Lokalisierung mithilfe der \textit{SLAM}-Methode gefordert, kann dieselbe Fahraufgabe mit einer sich aufbauenden Karte vollzogen werden. Widerrum schließen sich diverse Zustände gegenseitig aus. Eine bestimmte Zielpose ist von dem Ursprung einer statischen Karte abhängig. Diese ist beim \textit{SLAM}-Algorithmus jedoch nicht vorhanden. So kann folglich kein Ziel angefahren werden, wenn zuvor die Lokalisierung mithilfe von \textit{SLAM} abgeschlossen wurde.\\
		
		\begin{figure}[H]
			\centering
			\begin{minipage}[b]{0.4\textwidth}
				\begin{equation}
					m_2(\epsilon_f)=\left\{\begin{array}{ll} 1 \text{ für } \epsilon_f=2,7 \\
					0 \text{ für }\epsilon_f\neq 2,7\end{array}\right. 
					\label{eq: m2}
				\end{equation}\\
			\end{minipage}
			\hfill
			\begin{minipage}[b]{0.55\textwidth}
				\begin{equation}
					m_3(\epsilon_f)=\left\{\begin{array}{ll} 1 \text{ für } \epsilon_f\neq2,7 \wedge m_2(\epsilon_f) = 0 \\
					0 \text{ für }\epsilon_f= 2,7\end{array}\right.
					\label{eq: m3}
				\end{equation}\\
			\end{minipage}
		\end{figure}
		
		Nur eines der Vektorelemente $m_2(\epsilon_f)$ und $m_3(\epsilon_f)$ kann aktiv sein. Der Modus entspricht auch hier dem Index und sagt aus, ob die Fahraufgabe mit einer statischen oder einer sich aufbauenden Karte bearbeitet werden soll. Hierfür gelten die bereits angesprochenen Restriktionen. Die drei fahrfähigen Endzustände \textit{Manuell}, \textit{Erkunden} und \textit{Ziel} werden durch $m_5$, $m_7$ und $m_8$ aktiviert. Der Zustandsautomat besitzt die Fähigkeit, diverse Zwischenzustände, als temporäre Endzustände $\epsilon_f$ auszuführen. Somit kann sich das ALF beispielsweise im Modus \textit{Statische Karte} $\epsilon_f=3$ ohne eine bestimmte Fahraufgabe selbst lokalisieren. In einem weiteren Schritt ist es dem Benutzer erlaubt, vollständige und fahrfähige Endzustände auszuwählen, wie zum Beispiel den Modus \textit{Ziel} $\epsilon_f=8$.\\
		
		
				\begin{equation}
					m_5(\epsilon_f)=\left\{\begin{array}{ll} 1 \text{ für } \epsilon_f=5 \\
						0 \text{ für }\epsilon_f\neq 5\end{array}\right. 
					\label{eq: m5}
				\end{equation}\\
			
		Auch bei den genannten, fahrfähigen Endzuständen gibt es verschiedene Einschränkungen. So kann der manuelle Fahrmodus nur ausgewählt werden, wenn er als Endzustand $\epsilon_f$ definiert wurde. Die entsprechende Logik ist in Gleichung \ref{eq: m5} gezeigt.\\
		
				\begin{equation}
					m_7(\epsilon_f)=\left\{\begin{array}{ll} 1 \text{ für } \epsilon_f=7 \wedge m_2(\epsilon_f)=1 \wedge m_5(\epsilon_f)=0 \\
						0 \text{ für }\epsilon_f\neq 7\end{array}\right.
					\label{eq: m7}
				\end{equation}\\
				
		Gleichung \ref{eq: m7} stellt die Restriktionen des Zustands \textit{Erkunden} dar. Wenn der Modus als Endzustand gewählt wurde, die Lokalisierung ausschließlich durch die \textit{SLAM}-Methode realisiert wird und zuvor kein fahrfähiger Endzustand aktiviert wurde, darf der Zustand \textit{Erkunden} in Kraft treten. Für den Zustand \textit{Ziel} gilt grundlegend dieselbe Logik für die Aktivierung. Jedoch verlangt der Fahrmodus eine statische Karte. In Gleichung \ref{eq: m8} sind die Abhängigkeiten genauer verdeutlicht.\\
		
		
				\begin{equation}
					m_8(\epsilon_f)=\left\{\begin{array}{ll} 1 \text{ für } \epsilon_f=8 \wedge m_3(\epsilon_f)=1 \wedge m_5(\epsilon_f)=0 \wedge m_7(z_f)=0 \\
						0 \text{ für }\epsilon_f\neq 8\end{array}\right.
					\label{eq: m8}
				\end{equation}\\
		
		Für eine übersichtliche Darstellung eines Zustandsautomats wird \textit{Unified Modeling Language} (UML) verwendet. Das dem entworfenen Zustandsautomaten entsprechende UML-Diagramm ist in Abbildung \ref{fig: uml} gezeigt. Zustände sind als Boxen mit Abrundungen dargestellt und durch den Namen sowie den dazugehörigen Index gekennzeichnet. Pfeile stehen für Transitionen zwischen den Zuständen. Je nach Hierarchieebene legen schwarze Punkte den Startpunkt fest, der durchlaufen werden muss. 
		
	
	
	
	\input{Bilder/uml} 
	
	Darüber hinaus gilt es, den Zustandsautomaten und das ALF nach gegebenen Standards auszulegen. Die \textit{Society of Automotive Engineers} (SAE) ist eine Organisation, die aus Ingenieuren und technischen Experten besteht \cite{saeorg}. Ziel dieser ist im Allgemeinen die Weiterentwicklung im Thema Mobilität \cite{saeorg}. Unter anderem veröffentlicht \textit{SAE International} Normen wie die \textit{SAE J3016}. Diese beschäftigt sich mit verschiedenen Stufen der Automatisierung von Fahrzeugen.In Abbildung \ref{fig: tabautonom} sind die Stufen dargestellt.\\
	
	Das ALF wird zum Zeitpunkt des Entwicklungsbeginns dieser Masterarbeit auf dem \textit{SAE-Level} 3 eingeschätzt. Hierbei werden lediglich die autonomen Fahrmodi des Fahrzeugs berücksichtigt. Die Fahrbefehle konnten selbstständig durch das System ausgeführt werden und die Umgebung über die bereits genannten Sensoren erfasst werden. Die \textit{Fallback Performance} beschreibt den Übergang des automatisierten oder autonomen in einen risikominimalen Zustand. Bisher musste die laufende Software des ALFs auch im Fehlerfall manuell am Fahrzeug beendet werden. Dies bestärkte die Einschätzung, dass das Fahrzeug bedingt automatisiert sei.	
	
	\input{Bilder/tabautonom}
	
	Das ALF wird im Rahmen der technischen und sicherheitsrelevanten Möglichkeiten auf das \textit{SAE-Level} 5 angehoben. Dies betrifft insbesondere den zu entwickelnden Zustandsautomat, da dieser den Aufruf der Fahraufgaben realisiert.\\
	  
	


		
	
		
		
				   		

