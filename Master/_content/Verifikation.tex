\chapter{Evaluation}
\label{ch: Verifikation}
Zur Evaluation der künstlichen neuronalen Netze wird eine anwendungsorientierte \textit{Benchmark} durchgeführt. Hierbei wird anhand der in Kapitel \ref{subsec: Entwickeltes neuronales Netz} beschriebenen Datensätze die \textit{Precision} und \textit{Recall} Methode angewendet. Weiterhin werden die Benchmarks auf einer Grafikkarte, dem integrierten Computer des ALFs und einem eingebettem System ausgeführt. Die Eckdaten der Grafikkarte sowie des ALFs sind in Anhang \ref{a: } präsentiert. Als eingebettetes System wird ein \textit{Raspberry Pi 3 Model B} verwendet. Es ist keine Veränderung der Genauigkeit je Netz auf den jeweiligen Geräten zu erwarten. Jedoch können so die Bearbeitungszeiten pro Bild für unterschiedliche Hardware verglichen werden. Eine präzise Auflistung aller gemessenen Analysezeiten ist in Tabelle \ref{fig: zeitentab} präsentiert. \\

Insgesamt werden für den Test durch \textit{COCO-Datensatz} 12755 Bilder aus dem Trainingsdatensatz verwendet. Der technische Hintergrund hierfür ist in Kapitel \ref{subsec: Entwickeltes neuronales Netz} zu finden. Als Vergleich analysiert jedes Netz auch den eigenen Datensatz. So kann die Performance am Einsatzort des ALFs an der Hochschule Bochum evaluiert werden. Jedes Bild wird für die verwendeten, neuronalen Netze auf eine Pixelgröße von $300 \times 300$ skaliert. Für die Evaluation des \textit{HoGs} wird eine höhere Auflösung gewählt. Hierbei wird eine Seite des Bildes softwareseitig auf 400 Pixel begrenzt.\\

Der in Grundlagenkapitel \ref{subsec: evaluation neuronaler netze} beschriebene mAP-Wert wird häufig auf Objekterkennungssystemen mit multiplen Klassen angewendet. Die hier entwickelte Personenerkennung soll jedoch lediglich die Klasse \textit{Person} erkennen. Somit ist der mAP-Wert in diesem Fall der Mittelwert eines Messwerts und kann als Integral der \textit{Precision-Recall}-Kurve angesehen werden. Im Verlauf der Evaluation der angewendeten Systeme wird mithilfe einer Berechnungssoftware jeweils der mAP-Wert berechnet.
 
\input{Bilder/hog1}

Die Kombination aus \textit{HoG} und \textit{SVM} erreicht in der Benchmark die in Abbildung \ref{fig: hog1} präsentierten Ergebnisse. Der \textit{mAP}-Wert liegt für den eigenen Datensatz bei ... und für den \textit{COCO}-Datensatz bei .... Bei der Durchsicht der eingetragenen Begrenzungsrahmen ist aufgefallen, dass diese verhältnismäßig groß ausfallen. Somit könnte der \textit{IoU}-Wert entsprechend niedrig sein und zu diesem Ergebnis führen. Diese Beobachtung kann der \textit{HoG-SVM}-Methode zugrunde liegen. Entsprechende Optimierungen wurden implementiert und sind in Abbildung \ref{hog1} gezeigt. 


\input{Bilder/cocossdmobilenetv1tflite}

Eine deutliche Steigerung hinsichtlich der Geschwindigkeit im Vergleich zur \textit{HoG-SVM}-Methode wird durch das quantisierte \textit{SSD MobileNet V1} Netz erreicht. Die Berechnung des Integrals der \textit{Precision-Recall}-Kurve ergab für den eigenen Datensatz einen Wert von 0,61 und für den \textit{COCO}-Datensatz 0,5. Dieses Netz ist in der Lage 90 verschiedene Klassen zu erkennen. Hierbei nimmt das Netz jedoch lediglich 4 MB Speicherplatz ein.

\input{Bilder/ownssdmobilenetv1}

Das in Abbildung \ref{fig: ownnetv1} gezeigte \textit{MobileNet V1 SSD}-Netz ist darauf trainiert Personen zu erkennen. Die Trainingskonfigurationen der trainierten Netze sind in Anhnag ... dargestellt. Die dargestellte Architektur weißt einen \textit{mAP}-Werte von ... für den eigenen Datzensatz und ... für den \textit{COCO}-Datensatz auf.

\input{Bilder/ownnetv2}

In Darstellung \ref{fig: ownnetv2} werden die \textit{Precision-Recall}-Kurven eines \textit{MobileNet V2 SSD} gezeigt. Das Verhaltensmuster dieses Netzes ist ebenfalls auf eine Personenerkennung beschränkt, um enthaltene Parameter und den damit verbundenen Speicherplatz zu reduzieren. Die \textit{mAP}-Werte    

\input{Bilder/ssdmobilenetv2tflite}

Abbildung \ref{fig: ssdmobilenetv2} zeigt die \textit{Precision} und \textit{Recall} Kurve einer \textit{MobileNet V2 SSDLite} Architektur. Auffällig hierbei, ist die konstant hohe Genauigkeit $p(t)$. Diese liegt bis zu einem \textit{Recall}-Wert von circa 0,8 zwischen 0,9 und 1. Für die Anwendung auf den eigenen Datensatz erreicht diese Architektur eine mittlere Durschnittsgenauigkeit von ... . Angewendet auf den \textit{COCO}-Datensatz liegt der Wert bei .... 

 
\input{Bilder/ownnetv2ssdlite}
 
  
Die \textit{Precision-Recall}-Kurven der \textit{MobileNet V2 SSDLite} Architektur sind in der Grafik \ref{fig: ownnetv2ssdlite} abgebildet. \\

Nachfolgend werden die Benchmarkergebnisse der untersuchten Objekterkennungssysteme gegenübergestellt. Der Vergleich dient zur Veranschaulichung der gemessenen \textit{Precision-Recall}-Leistung je Datensatz. In Abbildung \ref{fig: genauigkeitsvergleich} werden die untersuchten \textit{MobileNet}-Architekturen anhand des eigenen Datensatzes verglichen. So kann eine Aussage darüber getroffen werden, ob die Systeme ortsabhängig ein anderes Verhaltensmuster an den Tag legen. Es kann beispielsweise passieren, dass die Netze aufgrund prägnater Eigenschaften der Umgebung am Standort der Hochschule Bochum verschieden reagieren. Eine allgemeine Aussage über die Genauigkeiten kann anhand der Darstellung \ref{fig: genauigkeitsvergleichcoco} getroffen werden. Für die Verallgemeinerung sorgt hierbei der \textit{COCO}-Datensatz. Dieser enthält anders als der eigene Datensatz Bilder von vielen verschiedenen Orten.     


\input{Bilder/gesamtvergleich}

\input{Bilder/gesamtvergleichcoco}

cocossdmobilenetv1cocotest

Im Gesamtvergleich der Genauigkeiten in Anwendung auf den eigenen Datensatz aller untersuchten Systeme sticht die ... Architektur als das Verfahren mit den höchsten \textit{Precision-Recall} Werten heraus. 

\input{Bilder/zeitenvergleich}

Die Berechnungszeit pro Bild des eigenen Datensatzes aller untersuchten Objekterkennungssysteme ist in der Tabelle in Abbildung \ref{fig: zeitentab} präsentiert. 

\input{Bilder/genauigkeiten}

